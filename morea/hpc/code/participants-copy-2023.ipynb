{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# import all the relevant libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport h5py\n\nimport keras\nfrom keras.datasets import cifar10\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Input, InputLayer, Dropout\n# This is erroring out, keras had an API change? \n#import keras.layers.merge as merge\n#from keras.layers.merge import Concatenate\nfrom tensorflow.keras.utils import to_categorical\n#from tensorflow.keras.optimizers import SGD, Adam\n\n%matplotlib inline",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# check for CPU and GPU for your session\n\nprint(tf.config.list_physical_devices('GPU'))\n\n# check for CPU here\nprint(tf.config.list_physical_devices('CPU'))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# load data and check the train-test split shape and size\n\n(x_train, y_train), (x_valid, y_valid) = cifar10.load_data()\nprint('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))\nprint('Test: X=%s, y=%s' % (x_valid.shape, y_valid.shape))\nprint('number of classes= %s' %len(set(y_train.flatten())))\nprint(type(x_train))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# print the data for the first data point (index 0)\ni = 0  # you can change this index to see a different \nprint(f\"Data Point {i + 1}:\")\nprint(\"Image Data:\")\nprint(x_train[i])  # print image data (32x32 pixels with 3 RGB channels)\nprint(\"Label:\")\nprint(y_train[i])  # print the label (one number representing the class)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# create figure and display the image of the first data point\ni = 0\nplt.figure()\nplt.imshow(x_train[i])\nplt.title(f\"Label: {y_train[i]}\")\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# specify classes from the cifar10 dataset\n\nnb_classes = 10\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# plotting some examples from training dataset\n\nplt.figure(figsize=(8, 8)) \nfor i in range(2*7):\n    # define subplot\n    plt.subplot(2, 7, i+1)\n    plt.imshow(x_train[i])\n    class_index = np.argmax(to_categorical(y_train[i], 10))\n    plt.title(class_names[class_index], fontsize=9)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# check TensorFlow library version\ntf.__version__",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# convert data to HDF5 format\n\nwith h5py.File('dataset_cifar10.hdf5', 'w') as hf:\n    dset_x_train = hf.create_dataset('x_train', data=x_train, shape=(50000, 32, 32, 3), compression='gzip', chunks=True)\n    dset_y_train = hf.create_dataset('y_train', data=y_train, shape=(50000, 1), compression='gzip', chunks=True)\n    dset_x_test = hf.create_dataset('x_valid', data=x_valid, shape=(10000, 32, 32, 3), compression='gzip', chunks=True)\n    dset_y_test = hf.create_dataset('y_valid', data=y_valid, shape=(10000, 1), compression='gzip', chunks=True)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Define the model\n\nmodel = tf.keras.Sequential()\nmodel.add(InputLayer(input_shape=[32, 32, 3]))\n\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=[2,2], strides=[2, 2], padding='same'))\n\nmodel.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=[2,2], strides=[2, 2], padding='same'))\n\nmodel.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=[2,2], strides=[2, 2], padding='same'))\n\nmodel.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=[2,2], strides=[2, 2], padding='same'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.summary()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Define the data generator\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    \n    def __init__(self, filename, batch_size, test=False, shuffle=True):\n        \n        self.PATH_TO_FILE = filename\n        \n        self.hf = h5py.File(self.PATH_TO_FILE, 'r')         \n        self.batch_size = batch_size\n        self.test = test\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __del__(self):\n        self.hf.close()\n        \n    def __len__(self):\n        return int(np.ceil(len(self.indices) / self.batch_size))\n\n    def __getitem__(self, idx):\n        start = self.batch_size * idx\n        stop = self.batch_size * (idx+1)\n        \n        if self.test:\n            x = self.hf['x_valid'][start:stop, ...]\n            batch_x = np.array(x).astype('float32') / 255.0\n            y = self.hf['y_valid'][start:stop]\n            batch_y = to_categorical(np.array(y), 10)\n        else:\n            x = self.hf['x_train'][start:stop, ...]\n            batch_x = np.array(x).astype('float32') / 255.0\n            y = self.hf['y_train'][start:stop]\n            batch_y = to_categorical(np.array(y), 10)\n\n        return batch_x, batch_y\n\n    def on_epoch_end(self):\n        if self.test:\n            self.indices = np.arange(self.hf['x_valid'][:].shape[0])\n        else:\n            self.indices = np.arange(self.hf['x_train'][:].shape[0])\n            \n        if self.shuffle:\n            np.random.shuffle(self.indices)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# generate batches of data for training and validation dataset\n\nfilename = \"dataset_cifar10.hdf5\"\nbatchsize  = 250 \ndata_train = DataGenerator(filename, batch_size=batchsize, test=False)\ndata_valid = DataGenerator(filename, batch_size=batchsize, test=True, shuffle=False)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# defining optimizer for the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# first, let's train the model using CPU\n\nwith tf.device('/device:CPU:0'):\n    history = model.fit(data_train,epochs=2,\n                        verbose=1, validation_data=data_valid)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# now, lets try with GPU to compare its performance with CPU\n\nfrom tensorflow.keras.models import clone_model\nnew_model = clone_model(model)\nopt = keras.optimizers.Adam(learning_rate=0.001)\nnew_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n## train the new_model using GPU here:\nwith tf.device('/device:GPU:0'):\n    new_history = new_model.fit(data_train,epochs=10, verbose=1, validation_data=data_valid)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# plotting the losses and accuracy for training and validation set\n\nfig, axes = plt.subplots(1,2, figsize=[16, 6])\naxes[0].plot(history.history['loss'], label='train_loss')\naxes[0].plot(history.history['val_loss'], label='val_loss')\naxes[0].set_title('Loss')\naxes[0].legend()\naxes[0].grid()\naxes[1].plot(history.history['accuracy'], label='train_acc')\naxes[1].plot(history.history['val_accuracy'], label='val_acc')\naxes[1].set_title('Accuracy')\naxes[1].legend()\naxes[1].grid()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "x = x_valid.astype('float32') / 255.0\ny = to_categorical(y_valid, 10)\nscore = new_model.evaluate(x, y, verbose=0)\nprint('Test cross-entropy loss: %0.5f' % score[0])\nprint('Test accuracy: %0.2f' % score[1])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "y_pred=model.predict(x) \ny_pred_classes=np.argmax(y_pred,axis=1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(8, 8)) \nfor i in range(24):\n    plt.subplot(4, 6, i+1)\n    plt.imshow(x[i].reshape(32,32,3))\n    index1 = np.argmax(y[i])\n    plt.title(\"y: %s\\np: %s\" % (class_names[index1], class_names[y_pred_classes[i]]), fontsize=9, loc='left')\n    plt.subplots_adjust(wspace=0.5, hspace=0.4)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Staging and File System Choice",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### Run this cell if you're not in your home directory",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cd",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Challenge 1: Locate Lustre and NFS File System Scratch On MANA",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "!pwd\n!ls",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Challenge 2: List Usage Information",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "!usage",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}